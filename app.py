# app.py â€” auto-load CSV & auto insights (no upload needed)
# -*- coding: utf-8 -*-
# âœ… ì¼ë°˜ì¸ë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì£¼ìš” ì „ë¬¸ìš©ì–´ ì˜†ì— ì‰¬ìš´ ì„¤ëª…(ì£¼ì„)ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.
# âœ… ê·¸ë˜í”„/í‘œ ì•„ë˜ì˜ ìë™ í•´ì„ ë¬¸ì¥ì„ ë” ìì„¸íˆ, ì‹¤ë¬´ ì‹œì‚¬ì ê¹Œì§€ ë‚˜ì˜¤ë„ë¡ ê°œì„ í–ˆìŠµë‹ˆë‹¤.

import os
import io
import pandas as pd
import numpy as np
import altair as alt
import matplotlib.pyplot as plt
import streamlit as st
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler  # í‘œì¤€í™”: ê° íŠ¹ì§•ì„ í‰ê· 0, í‘œì¤€í¸ì°¨1ë¡œ ë§ì¶° ë¹„êµ ì‰½ê²Œ í•¨
from sklearn.metrics import (
    classification_report,   # ì •ë°€ë„/ì¬í˜„ìœ¨/F1/ì •í™•ë„ ë“± ìš”ì•½í‘œ
    confusion_matrix,        # í˜¼ë™í–‰ë ¬: ì˜ˆì¸¡ê³¼ ì‹¤ì œê°€ ë§/í‹€ë¦° ê°œìˆ˜ í‘œ
    roc_auc_score,           # AUC: 0~1, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ì€ ë¶„ë¥˜ ì„±ëŠ¥(ë©´ì )
    RocCurveDisplay,         # ROC ê³¡ì„ : ë¯¼ê°ë„(ì¬í˜„ìœ¨)ì™€ ìœ„ì–‘ì„±ë¥  ê´€ê³„
)
from sklearn.linear_model import LogisticRegression   # ë¡œì§€ìŠ¤í‹± íšŒê·€: í™•ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ì„ í˜• ë¶„ë¥˜ê¸°
from sklearn.ensemble import RandomForestClassifier   # ëœë¤í¬ë ˆìŠ¤íŠ¸: ì—¬ëŸ¬ ê²°ì •íŠ¸ë¦¬ë¥¼ í•©ì³ ì˜ˆì¸¡í•˜ëŠ” ì•™ìƒë¸”

st.set_page_config(page_title="Software Defect Dataset Explorer", layout="wide")

# -----------------------
# Locate CSV automatically (same folder preferred)
# -----------------------
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
CSV_CANDIDATES = [
    os.path.join(SCRIPT_DIR, "software_defects_multilang_ast_1000.csv"),
    os.path.join(SCRIPT_DIR, "data", "software_defects_multilang_ast_1000.csv"),
    "/mnt/data/software_defects_multilang_ast_1000.csv",
]

def _first_existing(paths):
    for p in paths:
        if os.path.exists(p):
            return p
    return None

CSV_PATH = _first_existing(CSV_CANDIDATES)

st.title("ğŸ§ª Software Defects (Multilang) â€” ì‹œê° ë¶„ì„ & ê°„ë‹¨ ëª¨ë¸")

# Small status banner at the top-right
status_col1, status_col2 = st.columns([1, 2])
with status_col2:
    if CSV_PATH:
        st.caption(f"ë°ì´í„° ì†ŒìŠ¤: `{os.path.basename(CSV_PATH)}` (ìë™ ë¡œë“œ)")
    else:
        st.caption("ë°ì´í„° ì†ŒìŠ¤: (ì—†ìŒ) â€” ê°™ì€ í´ë”ì— CSVë¥¼ ë‘ë©´ ìë™ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤.")

@st.cache_data(show_spinner=True)
def load_data_from_path(path: str) -> pd.DataFrame:
    # ë©”ëª¨ë¦¬ì— ì½ì–´ì™€ì„œ Pandasë¡œ íŒŒì‹±
    with open(path, "rb") as f:
        buf = io.BytesIO(f.read())
    df = pd.read_csv(buf)
    return df

# -----------------------
# Glossary (ì‰¬ìš´ ë§ ìš©ì–´ ì„¤ëª…)
# -----------------------

def glossary_md() -> str:
    return (
        """
**ìš©ì–´ ì„¤ëª… (ì‰¬ìš´ ë§)**
- **ê²°í•¨(defect)**: ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ(ë²„ê·¸ê°€ ìˆì„ í™•ë¥ ì´ ë†’ë‹¤ê³  í‘œì‹œëœ í•­ëª©)
- **LOC(lines_of_code)**: ì½”ë“œ ì¤„ ìˆ˜. ê¸¸ìˆ˜ë¡ ë³µì¡í•´ì§ˆ ìˆ˜ ìˆìŒ
- **ìˆœí™˜ ë³µì¡ë„(cyclomatic_complexity)**: ë¶„ê¸°(If/ë°˜ë³µ) ë“±ìœ¼ë¡œ ë³µì¡í•œ ì •ë„ë¥¼ ìˆ˜ì¹˜ë¡œ í‘œí˜„
- **í† í° ìˆ˜(token_count)**: ì½”ë“œ ë‹¨ìœ„ë¥¼ ìª¼ê°  ìµœì†Œ ë‹¨ìœ„ ê°œìˆ˜(ê¸¸ì´/ë³µì¡ë„ì˜ ë‹¤ë¥¸ í‘œí˜„)
- **if/return/í•¨ìˆ˜í˜¸ì¶œ ìˆ˜**: ê°ê° ì¡°ê±´ë¬¸/ë°˜í™˜/ë‹¤ë¥¸ í•¨ìˆ˜ ë¶€ë¥´ëŠ” íšŸìˆ˜
- **AST ë…¸ë“œ ìˆ˜(ast_nodes)**: ì½”ë“œë¥¼ íŠ¸ë¦¬ë¡œ í‘œí˜„í–ˆì„ ë•Œì˜ ìš”ì†Œ ê°œìˆ˜(êµ¬ì¡°ì  ë³µì¡ë„)
- **í‘œë³¸ ìƒ˜í”Œ(sample)**: ë°ì´í„°ì˜ í•œ í–‰(í•˜ë‚˜ì˜ í•¨ìˆ˜/ì½”ë“œ ì¡°ê°)
- **í´ë˜ìŠ¤ ë¶ˆê· í˜•**: ê²°í•¨(1)ê³¼ ì •ìƒ(0)ì˜ ë¹„ìœ¨ ì°¨ì´ê°€ í° ìƒíƒœ
- **ìƒê´€ê´€ê³„**: ë‘ ìˆ˜ì¹˜ê°€ ê°™ì´ ì˜¤ë¥´ë‚´ë¦¬ëŠ” ê²½í–¥(ì¸ê³¼ê´€ê³„ì™€ëŠ” ë‹¤ë¦„)
- **ROC/AUC**: ì„ê³„ê°’ì„ ë°”ê¿”ê°€ë©° ì‚´í´ë³¸ ë¶„ë¥˜ ì„±ëŠ¥ ê³¡ì„ /ë©´ì (0.5=ìš´, 0.7~0.8 ë¬´ë‚œ, 0.8~0.9 ì–‘í˜¸)
- **ì •ë°€ë„(Precision)**: ê²°í•¨ì´ë¼ê³  í•œ ê²ƒ ì¤‘ ì‹¤ì œ ê²°í•¨ ë¹„ìœ¨(ê±°ì§“ ê²½ë³´ë¥¼ ì–¼ë§ˆë‚˜ ì¤„ì˜€ë‚˜)
- **ì¬í˜„ìœ¨(Recall)**: ì‹¤ì œ ê²°í•¨ ì¤‘ ì°¾ì•„ë‚¸ ë¹„ìœ¨(ë†“ì¹œ ê²°í•¨ì´ ì–¼ë§ˆë‚˜ ì ì€ê°€)
- **F1**: ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ê· í˜• ì§€í‘œ
- **í˜¼ë™í–‰ë ¬**: ì˜ˆì¸¡ê³¼ ì‹¤ì œì˜ ë§ì¶¤/í‹€ë¦¼ì„ í‘œë¡œ ìš”ì•½
- **í‘œì¤€í™”(Standardization)**: íŠ¹ì§•ë“¤ì„ ê°™ì€ ìŠ¤ì¼€ì¼ë¡œ ë§ì¶¤(ê³µì •í•œ ë¹„êµ)
- **ë¡œì§€ìŠ¤í‹± íšŒê·€**: ì„ í˜• ë°©ì‹ìœ¼ë¡œ ê²°í•¨ì¼ í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ ëª¨ë¸
- **ëœë¤í¬ë ˆìŠ¤íŠ¸**: ì—¬ëŸ¬ ê²°ì •ë‚˜ë¬´ë¥¼ ë¬¶ì–´ ê³¼ì í•©ì„ ì¤„ì´ê³  ì„±ëŠ¥ì„ ë†’ì´ëŠ” ëª¨ë¸
        """
    )

with st.expander("â„¹ï¸ ìš©ì–´ ì„¤ëª… ì—´ê¸°(ì´ˆë³´ììš©)"):
    st.markdown(glossary_md())

# -----------------------
# Load data or stop with a helpful message
# -----------------------
if not CSV_PATH:
    st.error(
        "CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `app.py`ì™€ ê°™ì€ í´ë”ì— "
        "`software_defects_multilang_ast_1000.csv`ë¥¼ ë‘ê³  ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”."
    )
    st.stop()

try:
    data = load_data_from_path(CSV_PATH)
except Exception as e:
    st.exception(e)
    st.stop()

st.caption("ë‹¤êµ­ì–´ í•¨ìˆ˜ ì½”ë“œì˜ ì •ì  ë¶„ì„ ì§€í‘œë¡œ ê²°í•¨(defect)ì„ íƒìƒ‰í•˜ê³  ë¶„ë¥˜ ëª¨ë¸ì„ ì‹œë„í•©ë‹ˆë‹¤.")

# -----------------------
# Basic validation
# -----------------------
required_cols = [
    "function_name","code","language","lines_of_code","cyclomatic_complexity",
    "token_count","num_ifs","num_returns","num_func_calls","ast_nodes","defect"
]
missing = [c for c in required_cols if c not in data.columns]
if missing:
    st.error(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}")
    st.stop()

# ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜(ì—ëŸ¬ëŠ” NaNìœ¼ë¡œ)
numeric_cols = [
    "lines_of_code","cyclomatic_complexity","token_count",
    "num_ifs","num_returns","num_func_calls","ast_nodes","defect"
]
for c in numeric_cols:
    data[c] = pd.to_numeric(data[c], errors="coerce")

data = data.dropna(subset=numeric_cols).copy()
if data.empty:
    st.error("ëª¨ë“  í–‰ì´ ëˆ„ë½ê°’ìœ¼ë¡œ ì œê±°ë˜ì–´ ë°ì´í„°ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# íƒ€ê¹ƒì€ 0/1 ì •ìˆ˜ë¡œ ë³´ì •
try:
    data["defect"] = data["defect"].astype(int)
except Exception:
    st.error("`defect` ì»¬ëŸ¼ì„ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 0/1ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# -----------------------
# Top metrics (ì‰½ê²Œ ì½íˆëŠ” í•µì‹¬ ìˆ«ì)
# -----------------------
col_a, col_b, col_c, col_d = st.columns(4)
col_a.metric("ì „ì²´ ìƒ˜í”Œ ìˆ˜", f"{len(data):,}")
col_b.metric("í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ìˆ˜", data["language"].nunique())
col_c.metric("ê²°í•¨ ë¹„ìœ¨(%)", f"{data['defect'].mean()*100:.1f}")
col_d.metric("í‰ê·  LOC", f"{data['lines_of_code'].mean():.2f}")

st.markdown("---")

# -----------------------
# Filters (sidebar)
# -----------------------
st.sidebar.header("í•„í„°")
langs = sorted(data["language"].unique().tolist())
sel_langs = st.sidebar.multiselect("ì–¸ì–´ ì„ íƒ", langs, default=langs)
loc_max = int(data["lines_of_code"].max())
loc_range = st.sidebar.slider("ë¼ì¸ ìˆ˜(LOC) ë²”ìœ„", 0, loc_max, (0, loc_max))
show_code = st.sidebar.checkbox("í‘œ ë¯¸ë¦¬ë³´ê¸°ì— code ì»¬ëŸ¼ í¬í•¨", value=False)

# í•„í„° ì ìš©
_df = data.query("language in @sel_langs").copy()
_df = _df[(_df["lines_of_code"] >= loc_range[0]) & (_df["lines_of_code"] <= loc_range[1])]

if _df.empty:
    st.info("í˜„ì¬ í•„í„°ë¡œëŠ” í–‰ì´ ì—†ìŠµë‹ˆë‹¤. í•„í„°ë¥¼ ì¡°ì •í•˜ì„¸ìš”.")
    st.stop()

st.subheader("ğŸ“„ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
preview_cols = [c for c in _df.columns if c != "code"] if not show_code else _df.columns
st.dataframe(_df[preview_cols].head(50), use_container_width=True)

# -----------------------
# Insight helpers (ìƒì„¸ í•´ì„)
# -----------------------

def _pct(n, d):
    return 0 if d == 0 else round(n / d * 100, 1)


def _skew_text(skew: float) -> str:
    if skew > 0.5:
        return "ë¶„í¬ì˜ ê¼¬ë¦¬ê°€ ì˜¤ë¥¸ìª½(í° ê°’)ìœ¼ë¡œ ê¸¸ì–´ í‰ê· ì´ ì¤‘ì•™ê°’ë³´ë‹¤ ì»¤ì§€ëŠ” ê²½í–¥"
    if skew < -0.5:
        return "ë¶„í¬ì˜ ê¼¬ë¦¬ê°€ ì™¼ìª½(ì‘ì€ ê°’)ìœ¼ë¡œ ê¸¸ì–´ í‰ê· ì´ ì¤‘ì•™ê°’ë³´ë‹¤ ì‘ì•„ì§€ëŠ” ê²½í–¥"
    return "ëŒ€ì²´ë¡œ ì¢Œìš° ëŒ€ì¹­ì— ê°€ê¹Œìš´ ë¶„í¬"


def insight_language_counts(df: pd.DataFrame) -> str:
    total = int(df.shape[0])
    vc = df["language"].value_counts()
    top = vc.head(3)
    parts = [
        "í‘œë³¸ ìˆ˜ ìƒìœ„ ì–¸ì–´: " + ", ".join([f"{idx}({_pct(cnt, total)}%)" for idx, cnt in top.items()])
    ]
    if vc.nunique() > 1:
        share = _pct(vc.iloc[0], total)
        if share >= 60:
            parts.append("íŠ¹ì • ì–¸ì–´ì— í‘œë³¸ì´ ë§ì´ ëª°ë ¤ ìˆì–´ ê²°ê³¼ê°€ ê·¸ ì–¸ì–´ì— ì¹˜ìš°ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ëŠ¥í•˜ë©´ í‘œë³¸ì„ ë³´ê°•í•˜ê±°ë‚˜ ê°€ì¤‘ì¹˜/êµì°¨ê²€ì¦ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        elif share <= 35:
            parts.append("ì–¸ì–´ë³„ í‘œë³¸ì´ ë¹„êµì  ê³ ë¥´ê²Œ ë¶„í¬í•˜ì—¬ ì¼ë°˜í™”ì— ìœ ë¦¬í•©ë‹ˆë‹¤.")
    parts.append("ìƒ˜í”Œ ë¶ˆê· í˜•ì€ ëª¨ë¸ì´ ì†Œìˆ˜ ì–¸ì–´ë¥¼ ê³¼ì†Œí•™ìŠµí•  ìœ„í—˜ì„ í‚¤ì›ë‹ˆë‹¤.")
    return "\n- " + "\n- ".join(parts)


def insight_class_ratio(df: pd.DataFrame) -> str:
    pos = int((df["defect"] == 1).sum())
    neg = int((df["defect"] == 0).sum())
    total = pos + neg
    pos_p = _pct(pos, total)
    parts = [
        f"ê²°í•¨(1) ë¹„ìœ¨: {pos_p}%(ê²°í•¨ {pos}ê°œ / ì •ìƒ {neg}ê°œ).",
        ("ì‹¬í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜• ê°€ëŠ¥ì„± â†’ í•™ìŠµ ì‹œ `class_weight='balanced'` ë˜ëŠ” ë¦¬ìƒ˜í”Œë§(SMOTE/ì–¸ë”ìƒ˜í”Œë§) ê³ ë ¤." if pos_p <= 30 or pos_p >= 70 else "ë¶ˆê· í˜•ì´ í¬ì§€ ì•Šì•„ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œë„ ë¬´ë‚œí•©ë‹ˆë‹¤."),
        "ì—…ë¬´ìƒ ì¤‘ìš”í•œ ì˜¤ë¥˜ ìœ í˜•ì´ ìˆë‹¤ë©´ ì¬í˜„ìœ¨(ë†“ì¹˜ì§€ ì•Šê¸°)ì„ ìš°ì„ í• ì§€, ì •ë°€ë„(ê±°ì§“ ê²½ë³´ ì¤„ì´ê¸°)ë¥¼ ìš°ì„ í• ì§€ ëª©í‘œë¥¼ ì •í•˜ì„¸ìš”."
    ]
    return "\n- " + "\n- ".join(parts)


def insight_histogram(df: pd.DataFrame, col: str) -> str:
    q1, q2, q3 = df[col].quantile([0.25, 0.5, 0.75])
    mean = df[col].mean()
    skew = float(df[col].skew())
    iqr = float(q3 - q1)
    parts = [
        f"ì¤‘ì•™ê°’ {q2:.2f}, í‰ê·  {mean:.2f} â†’ {_skew_text(skew)}.",
        f"ì‚¬ë¶„ìœ„ ë²”ìœ„(IQR) {iqr:.2f}. IQR ê¸°ì¤€ìœ¼ë¡œ ì´ìƒì¹˜ê°€ ë§ë‹¤ë©´ í’ˆì§ˆ ê·œì¹™/ì½”ë“œ ìŠ¤íƒ€ì¼ ì ê²€ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    ]
    if iqr > 0 and (df[col] > q3 + 1.5 * iqr).any():
        parts.append("ìƒí•œ(Upper fence) ë°–ì˜ í° ê°’ ì´ìƒì¹˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤ â†’ ê³¼ë„í•œ ë³µì¡ë„/ê¸´ í•¨ìˆ˜ ê°€ëŠ¥ì„±.")
    if iqr > 0 and (df[col] < q1 - 1.5 * iqr).any():
        parts.append("í•˜í•œ(Lower fence) ë°–ì˜ ì‘ì€ ê°’ ì´ìƒì¹˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤ â†’ ìë™ ìƒì„±/í…œí”Œë¦¿ ì½”ë“œ ì—¬ë¶€ í™•ì¸.")
    return "\n- " + "\n- ".join(parts)


def insight_box_by_lang(df: pd.DataFrame, col: str) -> str:
    med = df.groupby("language")[col].median().sort_values(ascending=False)
    var = df.groupby("language")[col].var().sort_values(ascending=False)
    parts = []
    if not med.empty:
        parts.append(f"ì¤‘ì•™ê°’ ìµœëŒ“ê°’: {med.index[0]} ({med.iloc[0]:.2f}) â†’ í•´ë‹¹ ì–¸ì–´ì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ {col}ê°€ ë†’ìŠµë‹ˆë‹¤.")
    if not var.empty and not np.isnan(var.iloc[0]):
        parts.append(f"ë¶„ì‚° ìµœëŒ“ê°’: {var.index[0]} ({var.iloc[0]:.2f}) â†’ ì–¸ì–´ë³„ í¸ì°¨ê°€ í° í¸ì…ë‹ˆë‹¤. í‘œì¤€/ê°€ì´ë“œ ì •ë¹„ë¥¼ ê²€í† í•˜ì„¸ìš”.")
    if not parts:
        parts.append("ì–¸ì–´ë³„ ì°¨ì´ê°€ í¬ì§€ ì•ŠìŠµë‹ˆë‹¤ â†’ ê³µí†µ ì •ì±…ìœ¼ë¡œ ê´€ë¦¬í•´ë„ ë¬´ë°©.")
    return "\n- " + "\n- ".join(parts)


def insight_corr(corr_df: pd.DataFrame) -> str:
    if "defect" not in corr_df.columns:
        return "- ê²°í•¨ ì—´ì´ ì—†ì–´ ìƒê´€ ë¶„ì„ì„ ìƒëµí–ˆìŠµë‹ˆë‹¤."
    s = corr_df["defect"].drop(labels=["defect"])  # type: ignore
    s_sorted = s.sort_values(ascending=False)
    s_abs_top = s.abs().sort_values(ascending=False).head(3)
    parts = [
        "ê²°í•¨(defect)ê³¼ì˜ ìƒê´€ ìƒìœ„ íŠ¹ì§•(ì ˆëŒ€ê°’ ê¸°ì¤€): " + ", ".join([f"{idx}(r={val:.2f})" for idx, val in s_abs_top.items()]),
        "ìƒê´€ì´ ë†’ë‹¤ê³  ì›ì¸ì´ ë˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤(ì¸ê³¼ì™€ëŠ” ë‹¤ë¦„). ë‹¤ë§Œ ê·œì¹™/ë¦¬ë·° í¬ì¸íŠ¸ë¡œ ìš°ì„  ê²€í† í•˜ê¸°ì— ì í•©í•©ë‹ˆë‹¤.",
    ]
    if not s_sorted.empty and s_sorted.iloc[0] > 0:
        parts.append("ì–‘(+)ì˜ ìƒê´€ì€ ê°’ì´ í´ìˆ˜ë¡ ê²°í•¨ì¼ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ëŠ” ëœ», ìŒ(-)ì˜ ìƒê´€ì€ ë°˜ëŒ€ì…ë‹ˆë‹¤.")
    return "\n- " + "\n- ".join(parts)


def insight_importances(imp_df: pd.DataFrame) -> str:
    if imp_df.empty:
        return "- ì¤‘ìš”ë„ ê³„ì‚° ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    top = imp_df.sort_values("importance", ascending=False).head(5)
    parts = [
        "ëª¨ë¸ì´ ì¤‘ìš”í•˜ê²Œ ë³¸ íŠ¹ì§•(ìƒìœ„ 5): " + ", ".join([f"{r.feature}({r.importance:.3f})" for r in top.itertuples()]),
        "ì¤‘ìš”ë„ëŠ” ë°ì´í„°ì™€ ëª¨ë¸ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì •ì±…/ë¦¬ë·° ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ ë•Œ ìƒìœ„ íŠ¹ì§•ë¶€í„° ë°˜ì˜í•´ ë³´ì„¸ìš”.",
    ]
    return "\n- " + "\n- ".join(parts)


def insight_roc(auc: float) -> str:
    if np.isnan(auc):
        return "- AUC ê³„ì‚° ë¶ˆê°€ (í™•ë¥  ì˜ˆì¸¡ ì—†ìŒ)."
    tier = (
        "ì–‘í˜¸" if auc >= 0.80 else ("ë¬´ë‚œ" if auc >= 0.70 else ("ê°œì„  í•„ìš”" if auc >= 0.60 else "ë‚®ìŒ"))
    )
    parts = [
        f"AUC={auc:.3f} â†’ ì„±ëŠ¥ ë“±ê¸‰: {tier}.",
        "AUCëŠ” ì„ê³„ê°’ ì „ë°˜ì˜ í‰ê· ì  ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì‹¤ì œ ì—…ë¬´ì—ì„œëŠ” ì •ë°€ë„Â·ì¬í˜„ìœ¨ì˜ ê· í˜•(F1)ì´ë‚˜ ì›í•˜ëŠ” ëª©í‘œì¹˜ì— ë§ì¶˜ ì„ê³„ê°’ ì¡°ì •ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
    ]
    return "\n- " + "\n- ".join(parts)


def insight_per_lang_table(grp: pd.DataFrame) -> str:
    if grp.empty:
        return "- ì–¸ì–´ë³„ ì§‘ê³„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    hi = grp.sort_values("defect_rate", ascending=False).iloc[0]
    lo = grp.sort_values("defect_rate", ascending=True).iloc[0]
    parts = [
        f"ê²°í•¨ë¥  ìµœê³ : {hi['language']} ({hi['defect_rate']:.2f}%) â†’ ìš°ì„  ê°œì„  ëŒ€ìƒ.",
        f"ê²°í•¨ë¥  ìµœì €: {lo['language']} ({lo['defect_rate']:.2f}%) â†’ ëª¨ë²” ì‚¬ë¡€ ë²¤ì¹˜ë§ˆí¬.",
    ]
    big = grp.sort_values("samples", ascending=False).iloc[0]
    parts.append(f"í‘œë³¸ ìµœë‹¤ ì–¸ì–´: {big['language']} ({int(big['samples'])}ê°œ) â†’ ì •ì±… ë³€ê²½ ì‹œ ì˜í–¥ë„ê°€ í¼.")
    return "\n- " + "\n- ".join(parts)

# -----------------------
# ë¶„í¬ & ì¹´ìš´íŠ¸ ì‹œê°í™”
# -----------------------
st.subheader("ğŸ“Š ë¶„í¬ & ì¹´ìš´íŠ¸")
left, right = st.columns(2)
with left:
    st.markdown("**ì–¸ì–´ë³„ ìƒ˜í”Œ ìˆ˜**")
    lang_count = _df["language"].value_counts().reset_index()
    lang_count.columns = ["language", "count"]
    chart1 = alt.Chart(lang_count).mark_bar().encode(
        x=alt.X("language:N", sort="-y"),
        y="count:Q",
        tooltip=["language", "count"],
    )
    st.altair_chart(chart1, use_container_width=True)
    st.markdown("**ê·¸ë˜í”„ í•´ì„**")
    st.markdown(insight_language_counts(_df))

with right:
    st.markdown("**ê²°í•¨(1) / ì •ìƒ(0) ë¹„ìœ¨**")
    cls_count = _df["defect"].value_counts().sort_index().rename_axis("defect").reset_index(name="count")
    cls_count["label"] = cls_count["defect"].map({0: "ì •ìƒ(0)", 1: "ê²°í•¨(1)"})
    chart2 = alt.Chart(cls_count).mark_arc(innerRadius=40).encode(
        theta="count:Q",
        color=alt.Color("label:N", legend=None),
        tooltip=["label", "count"],
    )
    st.altair_chart(chart2, use_container_width=True)
    st.markdown("**ê·¸ë˜í”„ í•´ì„**")
    st.markdown(insight_class_ratio(_df))

st.markdown("---")

# -----------------------
# Feature Explorer
# -----------------------
st.subheader("ğŸ§­ íŠ¹ì§•(Feature) íƒìƒ‰")
feature_cols = [
    "lines_of_code","cyclomatic_complexity","token_count",
    "num_ifs","num_returns","num_func_calls","ast_nodes"
]
feat = st.selectbox("ë¶„í¬ í™•ì¸í•  ìˆ˜ì¹˜ ì»¬ëŸ¼", feature_cols, index=0)

hist = alt.Chart(_df).mark_bar(opacity=0.8).encode(
    x=alt.X(f"{feat}:Q", bin=alt.Bin(maxbins=30)),
    y="count()",
    tooltip=[feat, alt.Tooltip("count()", title="count")],
)
st.altair_chart(hist, use_container_width=True)
st.markdown("**ê·¸ë˜í”„ í•´ì„**")
st.markdown(insight_histogram(_df, feat))

box = alt.Chart(_df).mark_boxplot().encode(
    x="language:N",
    y=alt.Y(f"{feat}:Q"),
    tooltip=["language", feat],
)
st.altair_chart(box, use_container_width=True)
st.markdown("**ê·¸ë˜í”„ í•´ì„**")
st.markdown(insight_box_by_lang(_df, feat))

st.markdown("---")

# -----------------------
# Correlation Heatmap
# -----------------------
st.subheader("ğŸ§ª ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ")
corr = _df[feature_cols + ["defect"]].corr(numeric_only=True)
corr_df = corr.reset_index().melt("index")
corr_df.columns = ["feature_x", "feature_y", "corr"]
heat = alt.Chart(corr_df).mark_rect().encode(
    x=alt.X("feature_x:N", sort=feature_cols + ["defect"]),
    y=alt.Y("feature_y:N", sort=feature_cols + ["defect"]),
    color=alt.Color("corr:Q", scale=alt.Scale(scheme="redyellowblue")),
    tooltip=["feature_x", "feature_y", alt.Tooltip("corr:Q", format=".2f")],
).properties(height=360)
st.altair_chart(heat, use_container_width=True)
st.markdown("**ê·¸ë˜í”„ í•´ì„**")
st.markdown(insight_corr(_df[feature_cols + ["defect"]].corr(numeric_only=True)))

st.markdown("---")

# -----------------------
# Simple Modeling
# -----------------------
st.subheader("ğŸ¤– ê²°í•¨ ì˜ˆì¸¡ (Quick Baselines)")
st.caption("ë¡œì§€ìŠ¤í‹± íšŒê·€/ëœë¤í¬ë ˆìŠ¤íŠ¸ë¡œ **ê¸°ì¤€ì„  ì„±ëŠ¥**ì„ ë¹ ë¥´ê²Œ í™•ì¸í•©ë‹ˆë‹¤. ì‹¤ì œ ì ìš© ì „, ë°ì´í„°/ëª©í‘œì— ë§ê²Œ ì„ê³„ê°’ê³¼ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ì„¸ìš”.")

# Features + target
X = _df[feature_cols].copy()
y = _df["defect"].copy()

# í´ë˜ìŠ¤ê°€ í•œìª½ë¿ì´ë©´ í•™ìŠµ ë¶ˆê°€
if y.nunique() < 2:
    st.warning("í˜„ì¬ í•„í„°ì—ì„œ íƒ€ê¹ƒ í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë¿ì…ë‹ˆë‹¤. ë‹¤ë¥¸ í•„í„°ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.")
    st.stop()

# ë°ì´í„° ë¶„í• (ê²€ì¦ì„ ìœ„í•´ ì¼ë¶€ë¥¼ í…ŒìŠ¤íŠ¸ë¡œ ë‚¨ê¹€)
test_size = st.slider("í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¹„ìœ¨", 0.1, 0.5, 0.2, 0.05)
random_state = st.number_input("random_state", min_value=0, value=42, step=1)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=random_state, stratify=y
)

# ëª¨ë¸ ì„ íƒ
model_name = st.selectbox("ëª¨ë¸ ì„ íƒ", ["LogisticRegression", "RandomForestClassifier"], index=1)

# í•™ìŠµ
if model_name == "LogisticRegression":
    scaler = StandardScaler()  # í‘œì¤€í™”: ê° íŠ¹ì§•ì„ ê°™ì€ ìŠ¤ì¼€ì¼ë¡œ ë§Œë“¤ì–´ ì„ í˜• ëª¨ë¸ì— ìœ ë¦¬
    X_train_s = scaler.fit_transform(X_train)
    X_test_s = scaler.transform(X_test)
    clf = LogisticRegression(max_iter=200)
    clf.fit(X_train_s, y_train)
    y_pred = clf.predict(X_test_s)
    y_proba = clf.predict_proba(X_test_s)[:, 1] if hasattr(clf, "predict_proba") else None
else:
    clf = RandomForestClassifier(
        n_estimators=200,
        max_depth=None,
        random_state=random_state,
        n_jobs=-1,
    )
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    y_proba = clf.predict_proba(X_test)[:, 1] if hasattr(clf, "predict_proba") else None

# ì§€í‘œ ê³„ì‚°
cm = confusion_matrix(y_test, y_pred)
report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
acc = report.get("accuracy", 0.0)                # ì •í™•ë„: ì „ì²´ ì¤‘ ë§ì¶˜ ë¹„ìœ¨
precision = report.get("1", {}).get("precision", 0.0)  # ì •ë°€ë„: ê²°í•¨ì´ë¼ê³  í•œ ê²ƒ ì¤‘ ì‹¤ì œ ê²°í•¨ ë¹„ìœ¨
recall = report.get("1", {}).get("recall", 0.0)        # ì¬í˜„ìœ¨: ì‹¤ì œ ê²°í•¨ ì¤‘ ì°¾ì•„ë‚¸ ë¹„ìœ¨
f1 = report.get("1", {}).get("f1-score", 0.0)          # F1: ì •ë°€ë„/ì¬í˜„ìœ¨ì˜ ê· í˜•
auc = roc_auc_score(y_test, y_proba) if y_proba is not None else float("nan")

m1, m2, m3, m4, m5 = st.columns(5)
m1.metric("Accuracy", f"{acc:.3f}")
m2.metric("Precision(Defect=1)", f"{precision:.3f}")
m3.metric("Recall(Defect=1)", f"{recall:.3f}")
m4.metric("F1(Defect=1)", f"{f1:.3f}")
m5.metric("ROC AUC", f"{auc:.3f}" if not np.isnan(auc) else "N/A")

# í˜¼ë™í–‰ë ¬ í‘œ
st.markdown("**Confusion Matrix (í˜¼ë™í–‰ë ¬)**")
cm_df = pd.DataFrame(cm, index=["True 0(ì •ìƒ)", "True 1(ê²°í•¨)"], columns=["Pred 0(ì •ìƒ)", "Pred 1(ê²°í•¨)"])
st.dataframe(cm_df, use_container_width=True)
st.markdown(
    "- ì¢Œìƒë‹¨: ì •ìƒìœ¼ë¡œ ë§ì¶¤, ìš°í•˜ë‹¨: ê²°í•¨ìœ¼ë¡œ ë§ì¶¤\n"
    "- ìš°ìƒë‹¨: ê±°ì§“ ê²½ë³´(False Positive), ì¢Œí•˜ë‹¨: ë†“ì¹œ ê²°í•¨(False Negative)"
)

# íŠ¹ì§• ì¤‘ìš”ë„(ëœë¤í¬ë ˆìŠ¤íŠ¸ë§Œ)
if hasattr(clf, "feature_importances_"):
    st.markdown("**Feature Importances (RandomForest)**")
    imp = pd.DataFrame({
        "feature": feature_cols,
        "importance": clf.feature_importances_,
    }).sort_values("importance", ascending=False)
    bar = alt.Chart(imp).mark_bar().encode(
        x=alt.X("importance:Q"),
        y=alt.Y("feature:N", sort="-x"),
        tooltip=["feature", alt.Tooltip("importance:Q", format=".4f")],
    )
    st.altair_chart(bar, use_container_width=True)
    st.markdown("**ê·¸ë˜í”„ í•´ì„**")
    st.markdown(insight_importances(imp))

# ROC Curve (í™•ë¥  ì˜ˆì¸¡ì´ ìˆì„ ë•Œ)
if y_proba is not None:
    st.markdown("**ROC Curve**")
    fig, ax = plt.subplots()
    RocCurveDisplay.from_predictions(y_test, y_proba, ax=ax)
    st.pyplot(fig, clear_figure=True)
    st.markdown("**ê·¸ë˜í”„ í•´ì„**")
    st.markdown(insight_roc(auc))

st.markdown("---")

# -----------------------
# Per-language breakdown
# -----------------------
st.subheader("ğŸ§© ì–¸ì–´ë³„ ì§€í‘œ")
grp = _df.groupby("language").agg(
    samples=("defect", "size"),
    defect_rate=("defect", "mean"),
    avg_loc=("lines_of_code", "mean"),
    avg_cc=("cyclomatic_complexity", "mean"),
    avg_tokens=("token_count", "mean"),
).reset_index()
grp["defect_rate"] = (grp["defect_rate"] * 100).round(2)
st.dataframe(grp, use_container_width=True)
st.markdown("**í‘œ í•´ì„**")
st.markdown(insight_per_lang_table(grp))

st.success("CSVë¥¼ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ì¢Œì¸¡ í•„í„°ì™€ ì˜µì…˜ì„ ë°”ê¿”ê°€ë©° íƒìƒ‰í•´ ë³´ì„¸ìš”!")
